{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e459d234",
   "metadata": {},
   "source": [
    "### 第10章: 事前学習済み言語モデル（GPT型）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7e5420",
   "metadata": {},
   "source": [
    "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68954ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamamoto/100knocks_2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トークン列 ['The', 'Ġmovie', 'Ġwas', 'Ġfull', 'Ġof']\n",
      "続くトークンの上位10語とその確率】\n",
      " 0.  jokes           尤度: 0.021892\n",
      " 1.  great           尤度: 0.018644\n",
      " 2.  laughs          尤度: 0.011524\n",
      " 3.  bad             尤度: 0.010874\n",
      " 4.  surprises       尤度: 0.010668\n",
      " 5.  references      尤度: 0.010528\n",
      " 6.  fun             尤度: 0.009992\n",
      " 7.  humor           尤度: 0.007415\n",
      " 8.  \"               尤度: 0.007408\n",
      " 9.  the             尤度: 0.006709\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "90. 次単語予測\n",
    "“The movie was full of”に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、\n",
    "その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。\n",
    "\"\"\"\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "text = \"The movie was full of\"\n",
    "encode_input = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "print(\"トークン列\",tokenizer.convert_ids_to_tokens(encode_input[\"input_ids\"][0]))\n",
    "\n",
    "\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encode_input)\n",
    "    logits = outputs.logits\n",
    "\n",
    "next_token_logits = logits[0, -1, :]\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "top_k = 10\n",
    "top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "print(\"続くトークンの上位10語とその確率】\")\n",
    "for i in range(top_k):\n",
    "    token = tokenizer.decode(top_indices[i].item())\n",
    "    prob = top_probs[i].item()\n",
    "    print(f\"{i:2d}. {token:<15}  尤度: {prob:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf47dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- temperature 0.7 ---\n",
      "テキスト 1: The movie was full of drama and suspense, but its main characters were not.\n",
      "\n",
      "\"We\n",
      "\n",
      "テキスト 2: The movie was full of twists and turns and a few memorable moments, including a sequence where a bunch\n",
      "\n",
      "テキスト 3: The movie was full of great characters and memorable moments. The opening scene of \"The Legend of Zelda\n",
      "\n",
      "--- temperature 1.0 ---\n",
      "テキスト 1: The movie was full of some truly weird and hilarious moments. We had to get into the character's\n",
      "\n",
      "テキスト 2: The movie was full of funny moments that made me want to watch it.\n",
      "\n",
      "Now, you\n",
      "\n",
      "テキスト 3: The movie was full of the best stuff. It really brought the whole universe together with one of my\n",
      "\n",
      "--- temperature 1.5 ---\n",
      "テキスト 1: The movie was full of jokes. Not for nothing did a female-friendly guy mention how, at\n",
      "\n",
      "テキスト 2: The movie was full of surprises! And as long as we had something great going (they always put\n",
      "\n",
      "テキスト 3: The movie was full of twists: When the titular human doctor is trapped in the middle of Hell\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "91. 続きのテキストの予測\n",
    "“The movie was full of”に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。\n",
    "\"\"\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "text = \"The movie was full of\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "temperatures = [0.7, 1.0, 1.5]\n",
    "num_texts = 3\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"--- temperature {temp} ---\")\n",
    "    with torch.no_grad():\n",
    "        input_ids = tokenizer.encode(\n",
    "            text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        attention_mask = torch.ones(input_ids.shape, device=model.device)\n",
    "\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_length=20,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=temp,\n",
    "            num_return_sequences=num_texts,\n",
    "            pad_token_id=tokenizer.eos_token_id, \n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "\n",
    "        for i, output in enumerate(output_ids):\n",
    "            generated_text = tokenizer.decode(output.tolist(), skip_special_tokens=True)\n",
    "            print(f\"テキスト {i+1}: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e548ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was full of に続くテキスト\n",
      " jokes           尤度: 0.021891\n",
      " and             尤度: 0.289224\n",
      " jokes           尤度: 0.098499\n",
      " about           尤度: 0.205555\n",
      " how             尤度: 0.099715\n",
      " the             尤度: 0.084637\n",
      " movie           尤度: 0.036412\n",
      " was             尤度: 0.296341\n",
      " a               尤度: 0.067677\n",
      " joke            尤度: 0.173508\n",
      "最終テキスト: The movie was full of jokes and jokes about how the movie was a joke\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "92. 予測されたテキストの確率を計算\n",
    "“The movie was full of”に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。\n",
    "\"\"\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "text = \"The movie was full of\"\n",
    "max_new_tokens = 10\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "print(f\"{text} に続くテキスト\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        next_token_logits = logits[0, -1, :] #最後のトークンの次のトークンに対するすべてのスコア\n",
    "        probs = F.softmax(next_token_logits, dim=-1) #確率に変換\n",
    "        top_probs, top_indices = torch.topk(probs, 1)\n",
    "\n",
    "        token = tokenizer.decode(top_indices[0].item())\n",
    "        prob = top_probs[0].item()\n",
    "        print(f\"{token:<15}  尤度: {prob:.6f}\")\n",
    "        input_ids = torch.cat([input_ids,top_indices[0].unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "\n",
    "print(f\"最終テキスト: {tokenizer.decode(input_ids[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b1a720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The movie was full of surprises, パープレキシティ: 99.35449981689453\n",
      "text: The movies were full of surprises, パープレキシティ: 126.48204040527344\n",
      "text: The movie were full of surprises, パープレキシティ: 278.88226318359375\n",
      "text: The movies was full of surprises, パープレキシティ: 274.6612854003906\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "93. パープレキシティ\n",
    "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
    "\n",
    "The movie was full of surprises\n",
    "\n",
    "The movies were full of surprises\n",
    "\n",
    "The movie were full of surprises\n",
    "\n",
    "The movies was full of surprises\n",
    "\n",
    "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "texts = [\"The movie was full of surprises\",\n",
    "         \"The movies were full of surprises\",\n",
    "         \"The movie were full of surprises\",\n",
    "         \"The movies was full of surprises\"]\n",
    "with torch.no_grad():\n",
    "    for text in texts:\n",
    "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        print(f\"text: {text}, パープレキシティ: {torch.exp(outputs.loss).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1ce9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:What do you call a sweet eaten after dinner?,Answer:Serves 4-6.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "94. チャットテンプレート\n",
    "“What do you call a sweet eaten after dinner?”という問いかけに対する応答を生成するため、\n",
    "チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。\n",
    "\"\"\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "question = \"What do you call a sweet eaten after dinner?\"\n",
    "prompt =f\"Question:{question},Answer:\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=True,top_k=50,top_p=0.95, temperature=0.7,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc42806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:What do you call a sweet eaten after dinner?,Answer:I like to eat sweet foods. I think it is a good thing to eat a lot of sweet foods. I think it is a good thing to eat a lot of sweet foods. I think it is a good thing to eat\n",
      "Question:Please give me the plural form of the word with its spelling in reverse order.,Answer:I am the plural form of the word with its spelling in reverse order.\n",
      "\n",
      "Answer:Please give me the plural form of the word with its spelling in reverse order.\n",
      "\n",
      "Answer:Please\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "95. マルチターンのチャット\n",
    "問題94で生成された応答に対して、追加で”Please give me the plural form of the word with its spelling in reverse order.”と問いかけたときの\n",
    "応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。\n",
    "\"\"\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "questions = [\"What do you call a sweet eaten after dinner?\",\n",
    "             \"Please give me the plural form of the word with its spelling in reverse order.\"]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for question in questions:\n",
    "        prompt =f\"Question:{question},Answer:\"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(input_ids, max_length=60, num_return_sequences=1, do_sample=True,top_k=40,top_p=0.9, temperature=0.5,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d130fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:11<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc : 0.4908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "96. プロンプトによる感情分析\n",
    "事前学習済み言語モデルで感情分析を行いたい。\n",
    "テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、\n",
    "SST-2の開発データにおける正解率を測定せよ。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def create_prompt(text, template = \"\"):\n",
    "    return f\"Please classify the sentiment of the following texts as either Positive or Negative:{template}Question:{text},Answer:\"\n",
    "\n",
    "few_shot = \"\"\"\n",
    "Question: \"hide new secretions from the parental units\"\n",
    "Answer: Negative\n",
    "Question: \"that loves its characters and communicates something rather beautiful about human nature\"\n",
    "Answer: Positive\n",
    "Question: \"contains no wit , only labored gags\"\n",
    "Answer: Negative\n",
    "Question: \"are more deeply thought through than in most ` right-thinking ' films.\"\n",
    "Answer: Positive\n",
    "\"\"\"\n",
    "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "correct = 0\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(0, len(dev_df), batch_size)):\n",
    "    batch_df = dev_df.iloc[i:i+batch_size]\n",
    "    batch_texts = batch_df[\"sentence\"].tolist()\n",
    "    prompts = [create_prompt(text, few_shot) for text in batch_texts]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 10,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    for j, response in enumerate(responses):\n",
    "        match = re.search(r\"Answer:\\s*(Positive|Negative)\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            answer = match.group(1).lower()\n",
    "            label = 1 if answer == \"positive\" else 0\n",
    "        else:\n",
    "            label = -1\n",
    "        \n",
    "        labels.append(label)\n",
    "        if label == batch_df.iloc[j][\"label\"]:\n",
    "            correct += 1\n",
    "\n",
    "print(f\"Acc : {correct / len(dev_df):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65370b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "97. 埋め込みに基づく感情分析\n",
    "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、\n",
    "そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"sentence\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = myDataset(train_df, tokenizer)\n",
    "dev_dataset = myDataset(dev_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cd4532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 1 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [13:11<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.2871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9106\n",
      "=================== 2 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [13:44<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.1772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9083\n",
      "=================== 3 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [13:45<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.1323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9197\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "class CustomGPT2Classifier(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\", num_labels=1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.gpt2 = GPT2Model.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.gpt2.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        last_token_indices = attention_mask.sum(dim=1) - 1\n",
    "        pooled = hidden_states[torch.arange(hidden_states.size(0)), last_token_indices]\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "    \n",
    "def train(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = pred.squeeze()\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = pred.squeeze()\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "\n",
    "model = CustomGPT2Classifier()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"=================== {epoch+1} / {max_epochs} epoch ===================\")\n",
    "    \n",
    "    avg_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "save_path = 'model/model97.pth' \n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa35c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "98. ファインチューニング\n",
    "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_prompt(text, label = \"\"):\n",
    "    return f\"Please classify the sentiment of the following texts as either Positive or Negative:Question:{text},Answer:{label}\"\n",
    "\n",
    "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "class myGPT2Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.texts = df[\"sentence\"].tolist()\n",
    "        self.labels_num = df[\"label\"].tolist()\n",
    "        self.labels = [\"Positive\" if label == 1 else \"Negative\" for label in self.labels_num]\n",
    "        self.prompts = [create_prompt(text, label) for text, label in zip(self.texts, self.labels)]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized = self.tokenizer(\n",
    "            self.prompts[idx],\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        input_ids = tokenized['input_ids'].squeeze(0)\n",
    "        attention_mask = tokenized['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': input_ids.clone()\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = myGPT2Dataset(train_df, tokenizer)\n",
    "dev_dataset = myGPT2Dataset(dev_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 1 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [17:08<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:38<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 89.33%\n",
      "\n",
      "=================== 2 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [17:11<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:38<00:00, 22.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 91.06%\n",
      "\n",
      "=================== 3 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [17:11<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 872/872 [00:38<00:00, 22.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 91.51%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].squeeze().long().to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dev_df, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for _, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Evaluating\"):\n",
    "        text = row['sentence']\n",
    "        gold_label = \"Positive\" if row['label'] == 1 else \"Negative\"\n",
    "\n",
    "        prompt = create_prompt(text)\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_length=input_ids.shape[1] + 10,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        try:\n",
    "            answer = decoded.split(\"Answer:\")[-1].strip().split()[0]\n",
    "        except:\n",
    "            answer = \"\"\n",
    "\n",
    "        if answer.lower() == gold_label.lower():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer)) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"=================== {epoch+1} / {max_epochs} epoch ===================\")\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_acc = evaluate(model, dev_df, tokenizer, device)\n",
    "    print(f\"Validation Accuracy: {val_acc * 100:.2f}%\\n\")\n",
    "\n",
    "save_path = 'model/model98.pth' \n",
    "torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please classify the sentiment of the following texts as either Positive or Negative:Question:this movie was amazing and made me cry,Answer:Positive\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "load_path = 'model/model98.pth'\n",
    "model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    text = \"this movie was amazing and made me cry\"\n",
    "    prompt = create_prompt(text)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad4dcb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:07<00:00, 9399.93 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 9217.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "99. 選好チューニング\n",
    "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、\n",
    "事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、\n",
    "近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "def preprocess_function(example):\n",
    "    prompt = create_prompt(example[\"sentence\"])\n",
    "    label_text = \"Positive\" if example[\"label\"] == 1 else \"Negative\"\n",
    "    rejected_text = \"Negative\" if example[\"label\"] == 1 else \"Positive\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": label_text,\n",
    "        \"rejected\": rejected_text\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function)\n",
    "dev_dataset = dev_dataset.map(preprocess_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import torch.distributed as dist\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "config = DPOConfig(\n",
    "    beta=0.1,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    output_dir=\"model\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    fsdp=[],\n",
    "    fsdp_config={},\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6946913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|██████████| 67349/67349 [00:32<00:00, 2082.40 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 67349/67349 [00:21<00:00, 3090.84 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 67349/67349 [02:06<00:00, 534.40 examples/s]\n",
      "Extracting prompt in eval dataset: 100%|██████████| 872/872 [00:00<00:00, 4544.70 examples/s]\n",
      "Applying chat template to eval dataset: 100%|██████████| 872/872 [00:00<00:00, 2844.89 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 872/872 [00:02<00:00, 319.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最終評価結果: {'eval_loss': 0.6931470632553101, 'eval_runtime': 14.971, 'eval_samples_per_second': 58.246, 'eval_steps_per_second': 0.935, 'eval_rewards/chosen': 1.9638468984339852e-07, 'eval_rewards/rejected': 5.5602651372055334e-08, 'eval_rewards/accuracies': 0.5122767686843872, 'eval_rewards/margins': 1.407820349186295e-07, 'eval_logps/chosen': -19.78765296936035, 'eval_logps/rejected': -61.800880432128906, 'eval_logits/chosen': -67.04113006591797, 'eval_logits/rejected': -62.113346099853516}\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"model/checkpoint-3159\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "config = DPOConfig(\n",
    "    beta=0.1,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    output_dir=\"model\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    fsdp=[],\n",
    "    fsdp_config={},\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"最終評価結果: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5017312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy: 87.96%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "def score(log_probs, input_ids, attention_mask):\n",
    "    token_log_probs = torch.gather(log_probs, 2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    token_log_probs = token_log_probs * attention_mask \n",
    "    return token_log_probs.sum(dim=1)\n",
    "\n",
    "def pairwise_acc(model, tokenizer, eval_dataset):\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(eval_dataset, batch_size=8)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            prompts = batch[\"prompt\"]\n",
    "            chosens = batch[\"chosen\"]\n",
    "            rejecteds = batch[\"rejected\"]\n",
    "\n",
    "            chosen_inputs = tokenizer([p + c for p, c in zip(prompts, chosens)],\n",
    "                                      return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            rejected_inputs = tokenizer([p + r for p, r in zip(prompts, rejecteds)],\n",
    "                                        return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "            chosen_inputs = {k: v.to(model.device) for k, v in chosen_inputs.items()}\n",
    "            rejected_inputs = {k: v.to(model.device) for k, v in rejected_inputs.items()}\n",
    "\n",
    "            chosen_outputs = model(**chosen_inputs)\n",
    "            rejected_outputs = model(**rejected_inputs)\n",
    "\n",
    "            chosen_log_probs = torch.nn.functional.log_softmax(chosen_outputs.logits, dim=-1)\n",
    "            rejected_log_probs = torch.nn.functional.log_softmax(rejected_outputs.logits, dim=-1)\n",
    "\n",
    "            chosen_score = score(chosen_log_probs, chosen_inputs[\"input_ids\"], chosen_inputs[\"attention_mask\"])\n",
    "            rejected_score = score(rejected_log_probs, rejected_inputs[\"input_ids\"], rejected_inputs[\"attention_mask\"])\n",
    "\n",
    "            correct += (chosen_score > rejected_score).sum().item()\n",
    "            total += len(prompts)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "model_dir = \"model/checkpoint-3159\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "acc = pairwise_acc(model, tokenizer, dev_dataset)\n",
    "\n",
    "print(f\"Pairwise Accuracy: {acc * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
