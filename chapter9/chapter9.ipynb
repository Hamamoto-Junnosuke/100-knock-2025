{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1a6d72",
   "metadata": {},
   "source": [
    "### 第9章: 事前学習済み言語モデル（BERT型）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aca088",
   "metadata": {},
   "source": [
    "本章では、BERT型の事前学習済みモデルを利用して、マスク単語の予測や文ベクトルの計算、評判分析器（ポジネガ分類器）の構築に取り組む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad5560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamamoto/100knocks_2025/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'movie', 'was', 'full', 'of', 'inc', '##omp', '##re', '##hen', '##si', '##bilities', '.']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "80. トークン化\n",
    "“The movie was full of incomprehensibilities.”という文をトークンに分解し、トークン列を表示せよ。\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"The movie was full of incomprehensibilities.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29d05f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【[MASK] に入る上位10語とその確率】\n",
      " 1. fun              尤度: 0.107119\n",
      " 2. surprises        尤度: 0.066345\n",
      " 3. drama            尤度: 0.044684\n",
      " 4. stars            尤度: 0.027217\n",
      " 5. laughs           尤度: 0.025413\n",
      " 6. action           尤度: 0.019517\n",
      " 7. excitement       尤度: 0.019038\n",
      " 8. people           尤度: 0.018290\n",
      " 9. tension          尤度: 0.015031\n",
      "10. music            尤度: 0.014646\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "81. マスクの予測\n",
    "“The movie was full of [MASK].”の”[MASK]”を埋めるのに最も適切なトークンを求めよ。\n",
    "\"\"\"\n",
    "from transformers import pipeline\n",
    "from pprint import pprint\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=model_name,device=0)\n",
    "text = \"The movie was full of [MASK].\"\n",
    "out_ten = pipe(text, top_k=10)\n",
    "print(\"【[MASK] に入る上位10語とその確率】\")\n",
    "for i, out in enumerate(out_ten, 1):\n",
    "    token = out[\"token_str\"]\n",
    "    score = out[\"score\"]\n",
    "    print(f\"{i:2d}. {token:<15}  尤度: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11098d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コサイン類似度\n",
      "文章1 : The movie was full of fun.\n",
      "文章2 : The movie was full of excitement.\n",
      "類似度 : 0.9880610108375549\n",
      "文章1 : The movie was full of fun.\n",
      "文章3 : The movie was full of crap.\n",
      "類似度 : 0.955765962600708\n",
      "文章1 : The movie was full of fun.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.9475324749946594\n",
      "文章2 : The movie was full of excitement.\n",
      "文章3 : The movie was full of crap.\n",
      "類似度 : 0.9541274905204773\n",
      "文章2 : The movie was full of excitement.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.9486637115478516\n",
      "文章3 : The movie was full of crap.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.9806932210922241\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "83. CLSトークンによる文ベクトル\n",
    "以下の文の全ての組み合わせに対して、最終層の[CLS]トークンの埋め込みベクトルを用いてコサイン類似度を求めよ。\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”\n",
    "\"\"\"\n",
    "from torch.nn.functional import cosine_similarity\n",
    "sim_enc_pipline = pipeline(\n",
    "    model = model_name,task=\"feature-extraction\"\n",
    ")\n",
    "texts = [\"The movie was full of fun.\",\n",
    "         \"The movie was full of excitement.\",\n",
    "         \"The movie was full of crap.\",\n",
    "         \"The movie was full of rubbish.\"]\n",
    "text_vecs = [sim_enc_pipline(text, return_tensors=True)[0][0] for text in texts]\n",
    "\n",
    "print(\"コサイン類似度\")\n",
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        print(f\"文章{i+1} : {texts[i]}\")\n",
    "        print(f\"文章{j+1} : {texts[j]}\")\n",
    "        sim_pair_score = cosine_similarity(text_vecs[i], text_vecs[j], dim=0)\n",
    "        print(f\"類似度 : {sim_pair_score.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fc6c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "コサイン類似度\n",
      "文章1 : The movie was full of fun.\n",
      "文章2 : The movie was full of excitement.\n",
      "類似度 : 0.956811249256134\n",
      "文章1 : The movie was full of fun.\n",
      "文章3 : The movie was full of crap.\n",
      "類似度 : 0.8489991426467896\n",
      "文章1 : The movie was full of fun.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.8168841600418091\n",
      "文章2 : The movie was full of excitement.\n",
      "文章3 : The movie was full of crap.\n",
      "類似度 : 0.8351833820343018\n",
      "文章2 : The movie was full of excitement.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.7938442230224609\n",
      "文章3 : The movie was full of crap.\n",
      "文章4 : The movie was full of rubbish.\n",
      "類似度 : 0.9225536584854126\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "84. 平均による文ベクトル\n",
    "以下の文の全ての組み合わせに対して、最終層の埋め込みベクトルの平均を用いてコサイン類似度を求めよ。\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”\n",
    "\"\"\"\n",
    "from torch.nn.functional import cosine_similarity\n",
    "sim_enc_pipline = pipeline(\n",
    "    model = model_name,task=\"feature-extraction\"\n",
    ")\n",
    "texts = [\"The movie was full of fun.\",\n",
    "         \"The movie was full of excitement.\",\n",
    "         \"The movie was full of crap.\",\n",
    "         \"The movie was full of rubbish.\"]\n",
    "text_vecs = []\n",
    "\n",
    "for text in texts:\n",
    "    vec = sim_enc_pipline(text, return_tensors=True)[0]\n",
    "    mean_vec = vec.mean(dim=0)\n",
    "    text_vecs.append(mean_vec)\n",
    "\n",
    "print(\"コサイン類似度\")\n",
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        print(f\"文章{i+1} : {texts[i]}\")\n",
    "        print(f\"文章{j+1} : {texts[j]}\")\n",
    "        sim_pair_score = cosine_similarity(text_vecs[i], text_vecs[j], dim=0)\n",
    "        print(f\"類似度 : {sim_pair_score.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c483645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor([0.]),\n",
      " 'text': 'hide new secretions from the parental units ',\n",
      " 'tokens': ['hide',\n",
      "            'new',\n",
      "            'secret',\n",
      "            '##ions',\n",
      "            'from',\n",
      "            'the',\n",
      "            'parental',\n",
      "            'units']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "85. データセットの準備\n",
    "General Language Understanding Evaluation (GLUE) ベンチマークで配布されているStanford Sentiment Treebank (SST) から\n",
    "訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、さらに全てのテキストはトークン列に変換せよ。\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"SST-2/train.tsv\", sep=\"\\t\")\n",
    "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "def text_to_token(df, tokenizer):\n",
    "    dct_lst = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        label = torch.tensor([float(row[\"label\"])])\n",
    "\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        \n",
    "        dct_lst.append({'text':sentence,\n",
    "            'label':label,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "           \n",
    "    return dct_lst\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_token_list  = text_to_token(train_df, tokenizer)\n",
    "dev_token_list = text_to_token(dev_df, tokenizer)\n",
    "pprint(train_token_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edb9cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "86. ミニバッチの作成\n",
    "85で読み込んだ訓練データの一部（例えば冒頭の4事例）に対して、パディングなどの処理を行い、トークン列の長さを揃えてミニバッチを構成せよ。\n",
    "\"\"\"\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def padding(token_list, tokenizer, max_length):\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    for dct in token_list:\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(dct['tokens'])\n",
    "\n",
    "        padded = input_ids[:max_length]\n",
    "        padded += [pad_token_id] * (max_length - len(padded))\n",
    "        attention_mask = [1 if id != pad_token_id else 0 for id in padded]\n",
    "\n",
    "        dct[\"input_ids\"] = torch.tensor(padded)\n",
    "        dct[\"attention_mask\"] = torch.tensor(attention_mask)\n",
    "    return token_list\n",
    "\n",
    "max_length = 64\n",
    "train_padded_token_list = padding(train_token_list, tokenizer, max_length=max_length)\n",
    "dev_padded_token_list = padding(dev_token_list, tokenizer, max_length=max_length)\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class tokenDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': item['input_ids'],\n",
    "            'attention_mask': item['attention_mask'],\n",
    "            'labels': item['label']\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "train_dataset = tokenDataset(train_padded_token_list)\n",
    "dev_dataset = tokenDataset(dev_padded_token_list)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=256, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed37bbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 1 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [06:11<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9163\n",
      "=================== 2 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [06:35<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9060\n",
      "=================== 3 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2105/2105 [06:41<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "87. ファインチューニング\n",
    "訓練セットを用い、事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = pred.logits.squeeze()\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = pred.logits.squeeze()\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_epochs = 3\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"=================== {epoch+1} / {max_epochs} epoch ===================\")\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "save_path = 'model/model87.pth' \n",
    "torch.save(model.state_dict(), save_path)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461c507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text : The movie was full of incomprehensibilities., 極性 : ネガティブ\n",
      "text : The movie was full of fun., 極性 : ポジティブ\n",
      "text : The movie was full of excitement, 極性 : ポジティブ\n",
      "text : The movie was full of crap., 極性 : ネガティブ\n",
      "text : The movie was full of rubbish., 極性 : ネガティブ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "88. 極性分析\n",
    "問題87でファインチューニングされたモデルを用いて、以下の文の極性を予測せよ。\n",
    "\n",
    "“The movie was full of incomprehensibilities.”\n",
    "\n",
    "“The movie was full of fun.”\n",
    "\n",
    "“The movie was full of excitement.”\n",
    "\n",
    "“The movie was full of crap.”\n",
    "\n",
    "“The movie was full of rubbish.”\n",
    "\"\"\"\n",
    "def analyze(text, tokenizer, model, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(**inputs)\n",
    "        logits = pred.logits.squeeze() \n",
    "        pred_label = torch.sigmoid(logits) > 0.5\n",
    "        print(f\"text : {text}, 極性 : {'ポジティブ' if pred_label else 'ネガティブ'}\")\n",
    "\n",
    "texts = [\"The movie was full of incomprehensibilities.\",\n",
    "         \"The movie was full of fun.\",\n",
    "         \"The movie was full of excitement\",\n",
    "         \"The movie was full of crap.\",\n",
    "         \"The movie was full of rubbish.\"]\n",
    "\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "load_path = 'model/model87.pth'\n",
    "model.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for text in texts:\n",
    "    analyze(text, tokenizer, model, device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75f0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== 1 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [07:13<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9128\n",
      "=================== 2 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [07:14<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9186\n",
      "=================== 3 / 3 epoch ===================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [07:15<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9186\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "89. アーキテクチャの変更\n",
    "問題87とは異なるアーキテクチャ（例えば[CLS]トークンを用いるか、各トークンの最大値プーリングを用いるなど）の分類モデルを設計し、\n",
    "事前学習済みモデルを極性分析タスク向けにファインチューニングせよ。検証セット上でファインチューニングされたモデルの正解率を計測せよ。\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.linear = nn.Linear(768, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state  # 最後の層の出力を受け取る\n",
    "        pooled = torch.max(hidden_states, dim=1).values  #最大値をとる\n",
    "        logits = self.linear(pooled) #線形層に通してスコアとして使えるようにする\n",
    "        return logits\n",
    "    \n",
    "def train(model, train_loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = pred.squeeze()\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, dev_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].squeeze().float().to(device)\n",
    "\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = pred.squeeze()\n",
    "            preds = torch.sigmoid(logits) > 0.5\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = CustomBERTModel(model_name, num_labels=1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_epochs = 3\n",
    "for epoch in range(max_epochs):\n",
    "    print(f\"=================== {epoch+1} / {max_epochs} epoch ===================\")\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    val_acc = evaluate(model, dev_loader, device)\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "save_path = 'model/model89.pth' \n",
    "torch.save(model.state_dict(), save_path)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
